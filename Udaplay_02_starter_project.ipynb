{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tavily import TavilyClient\n",
    "from typing import Dict\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.tooling import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "\n",
    "embedding_fn = embedding_functions.OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\n",
    "chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "collection = chroma_client.get_collection(name=\"udaplay\", embedding_function=embedding_fn)\n",
    "# Need to pass embedding function so dimensions match\n",
    "# The collection was created with an OpenAI embedding model, which has a dimension size of 1536.\n",
    "# However, it appears that Chroma uses an embedding model with a dimension size of 384 to embed the query.\n",
    "# So, if the embedding_function is not passed to get_collection(),  and then\n",
    "# the query() method is called on the collection, this will result in:\n",
    "# chromadb.errors.InvalidArgumentError: Collection expecting embedding with dimension of 1536, got 384\n",
    "# Essentially, the dimensions don't match, and passing embedding_function to get_collections() fixes this.\n",
    "\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game\n",
    "\n",
    "@tool\n",
    "def retrieve_game(query: str):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=['documents', 'metadatas']\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec961703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question1 = \"I like to drive. What games would I like?\"\n",
    "# question2 = \"What is the most recent version of Tetris, and when did it come out?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22460881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek() # confirm collection has been successfully retrieved\n",
    "# answer = retrieve_game(question1)\n",
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result\n",
    "\n",
    "\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "    Your task is to evaluate if the documents are enough to respond to the query. \n",
    "    \n",
    "    OUTPUT FORMAT:\n",
    "    \n",
    "    Your answer should consist of two parts:\n",
    "    1. A boolean value for whether the documents are sufficient to respond to the query.\n",
    "    2. A detailed explanation of your reasoning\n",
    "    \n",
    "    Provide your response as a JSON object in the following format:\n",
    "    \n",
    "    {\n",
    "        \"sufficient\": \"True\" | \"False\",\n",
    "        \"reasoning\": \" ... \"\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "@tool\n",
    "def evaluate_retrieval(question: str, retrieved_docs: list[str]):\n",
    "    \n",
    "    input = f\"\"\"\n",
    "\n",
    "    QUERY: {question}\n",
    "    \n",
    "    DOCUMENTS: {retrieved_docs}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        instructions=evaluation_prompt,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=input\n",
    "    )\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc71845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# print(evaluate_retrieval(question1, answer['documents']))\n",
    "# print()\n",
    "\n",
    "# eval = evaluate_retrieval(question2, answer['documents'])\n",
    "# eval_dict = json.loads(eval)\n",
    "# print(eval_dict)\n",
    "# print(eval_dict[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. \n",
    "\n",
    "@tool\n",
    "def web_search(query: str, search_depth: str = \"advanced\") -> Dict:\n",
    "    \"\"\"\n",
    "    Search the web using Tavily API\n",
    "    args:\n",
    "        query (str): Search query\n",
    "        search_depth (str): Type of search - 'basic' or 'advanced' (default: advanced)\n",
    "    \"\"\"\n",
    "    client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "    \n",
    "    # Perform the search\n",
    "    search_result = client.search(\n",
    "        query=query,\n",
    "        search_depth=search_depth,\n",
    "        include_answer=True,\n",
    "        include_raw_content=False,\n",
    "        include_images=False\n",
    "    )\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = {\n",
    "        \"answer\": search_result.get(\"answer\", \"\"),\n",
    "        \"results\": search_result.get(\"results\", []),\n",
    "        \"search_metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc90a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = web_search(question1)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeab24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional, Union, TypeVar\n",
    "import json\n",
    "\n",
    "from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run\n",
    "from lib.llm import LLM\n",
    "from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage\n",
    "from lib.tooling import Tool, ToolCall\n",
    "from lib.memory import ShortTermMemory\n",
    "\n",
    "# Define the state schema\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str  # The current user query being processed\n",
    "    instructions: str  # System instructions for the agent\n",
    "    messages: List[dict]  # List of conversation messages\n",
    "    current_tool_calls: Optional[List[ToolCall]]  # Current pending tool calls\n",
    "    total_tokens: int  # Track the cumulative total\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 instructions: str, \n",
    "                 tools: List[Tool] = None,\n",
    "                 temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize an Agent\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name/identifier of the LLM model to use\n",
    "            instructions: System instructions for the agent\n",
    "            tools: Optional list of tools available to the agent\n",
    "            temperature: Temperature parameter for LLM (default: 0.7)\n",
    "        \"\"\"\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools if tools else []\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Initialize memory and state machine\n",
    "        self.memory = ShortTermMemory()\n",
    "        self.workflow = self._create_state_machine()\n",
    "\n",
    "    def _prepare_messages_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        \n",
    "        # If no messages exist, start with system message\n",
    "        if not messages:\n",
    "            messages = [SystemMessage(content=state[\"instructions\"])]\n",
    "            \n",
    "        # Add the new user message\n",
    "        messages.append(UserMessage(content=state[\"user_query\"]))\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"session_id\": state[\"session_id\"]\n",
    "        }\n",
    "\n",
    "    def _llm_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "        # Initialize LLM\n",
    "        llm = LLM(\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "        current_total = state.get(\"total_tokens\", 0)\n",
    "        if response.token_usage:\n",
    "            current_total += response.token_usage.total_tokens\n",
    "\n",
    "        # Create AI message with content and tool calls\n",
    "        ai_message = AIMessage(\n",
    "            content=response.content, \n",
    "            tool_calls=tool_calls,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"current_tool_calls\": tool_calls,\n",
    "            \"session_id\": state[\"session_id\"],\n",
    "            \"total_tokens\": current_total,\n",
    "        }\n",
    "\n",
    "    def _tool_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "        tool_calls = state[\"current_tool_calls\"] or []\n",
    "        tool_messages = []\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            # Access tool call data correctly\n",
    "            function_name = call.function.name\n",
    "            function_args = json.loads(call.function.arguments)\n",
    "            tool_call_id = call.id\n",
    "            # Find the matching tool\n",
    "            tool = next((t for t in self.tools if t.name == function_name), None)\n",
    "            if tool:\n",
    "                result = str(tool(**function_args))\n",
    "                tool_message = ToolMessage(\n",
    "                    content=json.dumps(result), \n",
    "                    tool_call_id=tool_call_id, \n",
    "                    name=function_name, \n",
    "                )\n",
    "                tool_messages.append(tool_message)\n",
    "        \n",
    "        # Clear tool calls and add results to messages\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + tool_messages,\n",
    "            \"current_tool_calls\": None,\n",
    "            \"session_id\": state[\"session_id\"]\n",
    "        }\n",
    "\n",
    "    def _create_state_machine(self) -> StateMachine[AgentState]:\n",
    "        \"\"\"Create the internal state machine for the agent\"\"\"\n",
    "        machine = StateMachine[AgentState](AgentState)\n",
    "        \n",
    "        # Create steps\n",
    "        entry = EntryPoint[AgentState]()\n",
    "        message_prep = Step[AgentState](\"message_prep\", self._prepare_messages_step)\n",
    "        llm_processor = Step[AgentState](\"llm_processor\", self._llm_step)\n",
    "        tool_executor = Step[AgentState](\"tool_executor\", self._tool_step)\n",
    "        termination = Termination[AgentState]()\n",
    "        \n",
    "        machine.add_steps([entry, message_prep, llm_processor, tool_executor, termination])\n",
    "        \n",
    "        # Add transitions\n",
    "        machine.connect(entry, message_prep)\n",
    "        machine.connect(message_prep, llm_processor)\n",
    "        \n",
    "        # Transition based on whether there are tool calls\n",
    "        def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "            \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "            if state.get(\"current_tool_calls\"):\n",
    "                return tool_executor\n",
    "            return termination\n",
    "        \n",
    "        machine.connect(llm_processor, [tool_executor, termination], check_tool_calls)\n",
    "        machine.connect(tool_executor, llm_processor)  # Go back to llm after tool execution\n",
    "        \n",
    "        return machine\n",
    "\n",
    "    def invoke(self, query: str, session_id: Optional[str] = None) -> Run:\n",
    "        \"\"\"\n",
    "        Run the agent on a query\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to process\n",
    "            session_id: Optional session identifier (uses \"default\" if None)\n",
    "            \n",
    "        Returns:\n",
    "            The final run object after processing\n",
    "        \"\"\"\n",
    "        session_id = session_id or \"default\"\n",
    "\n",
    "        # Create session if it doesn't exist\n",
    "        self.memory.create_session(session_id)\n",
    "\n",
    "        # Get previous messages from last run if available\n",
    "        previous_messages = []\n",
    "        last_run: Run = self.memory.get_last_object(session_id)\n",
    "        if last_run:\n",
    "            last_state = last_run.get_final_state()\n",
    "            if last_state:\n",
    "                previous_messages = last_state[\"messages\"]\n",
    "\n",
    "        initial_state: AgentState = {\n",
    "            \"user_query\": query,\n",
    "            \"instructions\": self.instructions,\n",
    "            \"messages\": previous_messages,\n",
    "            \"current_tool_calls\": None,\n",
    "            \"session_id\": session_id,\n",
    "        }\n",
    "\n",
    "        run_object = self.workflow.run(initial_state)\n",
    "        \n",
    "        # Store the complete run object in memory\n",
    "        self.memory.add(run_object, session_id)\n",
    "        \n",
    "        return run_object\n",
    "\n",
    "    def get_session_runs(self, session_id: Optional[str] = None) -> List[Run]:\n",
    "        \"\"\"Get all Run objects for a session\n",
    "        \n",
    "        Args:\n",
    "            session_id: Optional session ID (uses \"default\" if None)\n",
    "            \n",
    "        Returns:\n",
    "            List of Run objects in the session\n",
    "        \"\"\"\n",
    "        return self.memory.get_all_objects(session_id)\n",
    "\n",
    "    def reset_session(self, session_id: Optional[str] = None):\n",
    "        \"\"\"Reset memory for a specific session\n",
    "        \n",
    "        Args:\n",
    "            session_id: Optional session to reset (uses \"default\" if None)\n",
    "        \"\"\"\n",
    "        self.memory.reset(session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed\n",
    "\n",
    "tools = [retrieve_game, evaluate_retrieval, web_search]\n",
    "\n",
    "instructions = f\"\"\"\n",
    "You are an expert on the video game industry.\n",
    "You can answer multistep questions by sequentially calling functions.\n",
    "You follow a pattern of of Thought and Action. \n",
    "Create a plan of execution: \n",
    "- Use Thought to describe your thoughts about the question you have been asked. \n",
    "- Use Action to specify one of the tools available to you. if you don't have a tool available, you can respond directly.\n",
    "Combine information from multiple sources if needed.\n",
    "\n",
    "When you think it's over, return the answer, listing all sources that you used.\n",
    "If you found the answer with the retrieve_game tool, provide the document used to arrive at your answer.\n",
    "If you used the web_search tool, list all links used to arrive at your answer.\n",
    "Never return an answer without listing your sources.\n",
    "\n",
    "Never try to respond directly if the question needs a tool. \n",
    "But if you don't have a tool available, you can respond directly. \n",
    "The actions you have are the Tools: {tools}.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    instructions=instructions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games: When Pokémon Gold and Silver was released?\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "Thought: I have retrieved information regarding the release date of Pokémon Gold and Silver. It was released in 1999 for the Game Boy Color. \n",
      "\n",
      "Action: I can now provide the final answer.\n",
      "\n",
      "Pokémon Gold and Silver was released in 1999 for the Game Boy Color.\n",
      "\n",
      "Sources:\n",
      "- Retrieved game details on Pokémon Gold and Silver.\n",
      "\n",
      "games: Which one was the first 3D platformer Mario game?\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "Thought: I have found that the first 3D platformer featuring Mario was Super Mario 64, which was released in 1996 for the Nintendo 64. This game was significant in shaping the 3D platforming genre.\n",
      "\n",
      "Action: I can now provide the final answer.\n",
      "\n",
      "The first 3D platformer Mario game was Super Mario 64, released in 1996 for the Nintendo 64.\n",
      "\n",
      "Sources:\n",
      "- Retrieved game details on Super Mario 64.\n",
      "\n",
      "games: Was Mortal Kombat X realeased for Playstation 5?\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "Thought: I found that Mortal Kombat X was originally released for PlayStation 4 in 2015, and it does not have a separate version released specifically for PlayStation 5. However, it is playable on the PS5 through backward compatibility.\n",
      "\n",
      "Action: I can now provide the final answer.\n",
      "\n",
      "Mortal Kombat X was not released specifically for PlayStation 5; it was released for PlayStation 4 in 2015. However, it can be played on PS5 via backward compatibility.\n",
      "\n",
      "Sources:\n",
      "- [PlayStation Store](https://store.playstation.com/en-us/product/UP1018-CUSA00967_00-MORTALKOMBATX000)\n",
      "- [Mortal Kombat X - Fandom](https://mortalkombat.fandom.com/wiki/Mortal_Kombat_X)\n",
      "- YouTube gameplay videos and reviews.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Invoke your agent\n",
    "queries = [\n",
    "    \"When Pokémon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X realeased for Playstation 5?\"\n",
    "]\n",
    "\n",
    "session_id = \"games\"\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"{session_id}: {q}\")\n",
    "    run_object = agent.invoke(query=q, session_id=session_id)\n",
    "    print(run_object.get_final_state()[\"messages\"][-1].content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
